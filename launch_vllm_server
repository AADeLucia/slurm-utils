#!/bin/bash
#
# launch_vllm_server
#
# A wrapper script to dynamically launch a vLLM OpenAI-compatible API server on Slurm.
#
# It waits for the job to start and then prints the necessary SSH tunnel
# command and an example python command to interact with the model endpoint.
#

# --- Source helper functions and configuration ---
# Assumes slurm_helpers.sh and config.sh are in the same directory or in the PATH
source slurm_helpers.sh
source config.sh

# --- Usage Instructions ---
usage() {
    echo "Usage: $0 --model <model_name> [--gpus <num_gpus>] [--time <HH:MM:SS>] [--model-max-length <len>]"
    echo "  --model <model_name>       : Required. The name of the model to host (e.g., 'meta-llama/Llama-2-7b-chat-hf')."
    echo "  --gpus <num_gpus>          : Optional. The number of GPUs to request. Defaults to 1."
    echo "  --time <HH:MM:SS>          : Optional. The wall time for the job. Defaults to 04:00:00."
    echo "  --model-max-length <len>   : Optional. The maximum sequence length for the model."
    exit 1
}

# --- Argument Parsing ---
NUM_GPUS=1                   # Default to a single GPU
MODEL_NAME=""                # Model name is required
TIME_LIMIT="04:00:00"        # Default to 4 hours
MODEL_MAX_LEN=""             # Default to empty (let vLLM decide)

# Loop through all command-line arguments
while [[ "$#" -gt 0 ]]; do
    case $1 in
        --gpus)
            NUM_GPUS="$2"
            shift 2
            ;;
        --model)
            MODEL_NAME="$2"
            shift 2
            ;;
        --time)
            TIME_LIMIT="$2"
            shift 2
            ;;
        --model-max-length)
            MODEL_MAX_LEN="$2"
            shift 2
            ;;
        -h|--help)
            usage
            ;;
        *)
            echo "Unknown parameter passed: $1"
            usage
            ;;
    esac
done

# --- Validate Input ---
if [ -z "$MODEL_NAME" ]; then
    echo "Error: Model name not provided."
    usage
fi

# --- Prepare Optional Flags ---
MAX_LEN_FLAG=""
if [ -n "$MODEL_MAX_LEN" ]; then
    MAX_LEN_FLAG="--max-model-len ${MODEL_MAX_LEN}"
fi

# --- Set Job Name from Model ---
# This makes it easy to identify which model is running in the queue.
JOB_NAME="vllm-$(basename ${MODEL_NAME})"

# --- Submit the job to Slurm using a 'here document' ---
SUB_MESSAGE=$(sbatch <<EOF
#!/bin/bash
#SBATCH --job-name=${JOB_NAME}
#SBATCH --partition=${GPU_PARTITION}
#SBATCH --account=${GPU_ACCOUNT}
#SBATCH --gres=gpu:${NUM_GPUS}
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8 # LLMs can benefit from more CPUs
#SBATCH --mem=64G         # Adjust memory as needed for larger models
#SBATCH --time=${TIME_LIMIT}
#SBATCH --output=${NOTEBOOK_LOGS}/%x.%j.log

# --- Load necessary modules ---
# Environment
module load anaconda3/2024.02-1
conda activate bmh

# Print environment name for logging
echo "Loaded modules and activated conda environment."
echo "Conda environment: \$(conda info --envs | grep '*' | awk '{print \$1}')"
echo "CUDA_VISIBLE_DEVICES: \${CUDA_VISIBLE_DEVICES}"

# --- Get a random open port on the compute node ---
PORT=\$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()')

# --- Get the name of the node where the job is running ---
NODE_HOSTNAME=\$(hostname -s)

# --- Print connection information to the log file ---
echo "================================================================="
echo "vLLM Server is starting on node: \${NODE_HOSTNAME} on port \${PORT}"
echo "Hosting model: ${MODEL_NAME}"
echo ""
echo "--> SSH TUNNEL COMMAND: ssh -N -L \${PORT}:\${NODE_HOSTNAME}:\${PORT} ${PUBLIC_LOGIN}"
echo "--> API BASE URL (localhost): http://localhost:\${PORT}/v1"
echo "--> API BASE URL (direct): http://\${NODE_HOSTNAME}:\${PORT}/v1"
echo ""
echo "Example Python command for testing (uses localhost URL, requires tunnel):"
echo "Make sure you have the openai library installed: pip install openai"
echo ""
echo "--> PYTHON EXAMPLE: python -c \"from openai import OpenAI; client = OpenAI(base_url='http://\${NODE_HOSTNAME}:\${PORT}/v1', api_key='not-needed'); print(client.completions.create(model='${MODEL_NAME}', prompt='San Francisco is a', max_tokens=7).choices[0].text)\""
echo "================================================================="
echo ""

# --- Start the vLLM OpenAI API-compatible Server ---
echo "Starting vLLM server..."
vllm serve "${MODEL_NAME}" \
  --port \${PORT} \
  --tensor-parallel-size "${NUM_GPUS}" ${MAX_LEN_FLAG} \
  --no-enable-log-requests

EOF
)

# --- Extract Job ID and print confirmation ---
JOB_ID=$(parse_job_id_from_submission_message "${SUB_MESSAGE}")
echo "✅ Job ${JOB_ID} submitted to Slurm to run vLLM."
echo "   Model: ${MODEL_NAME}"
echo "   GPUs: ${NUM_GPUS}"
echo "   Time Limit: ${TIME_LIMIT}"
if [ -n "$MODEL_MAX_LEN" ]; then
    echo "   Max Model Length: ${MODEL_MAX_LEN}"
fi
echo "   Waiting for job to start to retrieve connection details..."

# --- Wait for job to start ---
if ! wait_for_job_start "${JOB_ID}"; then
    echo "❌ Job failed to start. It may have been cancelled or encountered an error."
    exit 1
fi

# --- Parse the log file for connection instructions ---
LOG_FILE="${NOTEBOOK_LOGS}/${JOB_NAME}.${JOB_ID}.log"
echo ""
echo "✅ Job started! Use the following to connect:"
echo "-----------------------------------------------------------------"

# Give the server a moment to start up and write to the log file.
sleep 10

# Parse the log file for the SSH command and the API URLs.
grep "SSH TUNNEL" "${LOG_FILE}" | sed 's/--> SSH TUNNEL COMMAND: //'
echo ""
echo "API URLs (use the direct URL only if on the same network as the compute node):"
grep "API BASE URL" "${LOG_FILE}" | sed 's/--> API BASE URL //'
echo ""
echo "Example Request (make sure 'pip install openai' is installed locally):"
grep "PYTHON EXAMPLE" "${LOG_FILE}" | sed 's/--> PYTHON EXAMPLE: //'
echo "-----------------------------------------------------------------"